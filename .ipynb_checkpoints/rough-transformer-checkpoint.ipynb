{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "ba58b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "e3e1fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer():\n",
    "    \n",
    "    def __init__(self, d_model, vocab, n_heads, n_stack):\n",
    "        if(d_model % n_heads != 0):\n",
    "            raise Exception(\"Model dimensions must be divisible by number of attention heads\")\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.n_stack = n_stack\n",
    "        self.vocab = sorted(set(vocab))\n",
    "        \n",
    "        self.d_head = int(self.d_model / self.n_heads)\n",
    "        self.d_head_t = torch.tensor(self.d_head)\n",
    "        self.ff_hidden_size = self.d_model*4\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        self.v_to_i = {ch:idx for (idx, ch) in enumerate(self.vocab)}\n",
    "        self.i_to_v = {idx:ch for (idx, ch) in enumerate(self.vocab)}\n",
    "        \n",
    "        self.encoder_params = [self.__generate_encoder_params() for _ in range(self.n_stack)]\n",
    "        self.decoder_params = [self.__generate_decoder_params() for _ in range(self.n_stack)]\n",
    "        self.embeddings = torch.randn((self.vocab_size, d_model), dtype=torch.float)\n",
    "        self.projection = torch.randn((d_model, self.vocab_size), dtype=torch.float)\n",
    "        self.params = [\n",
    "            *[p for param_group in self.encoder_params for p in param_group],\n",
    "            *[p for param_group in self.decoder_params for p in param_group],\n",
    "            self.embeddings,\n",
    "            self.projection\n",
    "        ]\n",
    "        for p in self.params:\n",
    "            p.requires_grad = True\n",
    "        self.n_params = sum([p.nelement() for p in self.params])\n",
    "        print(f\"Transformer created with {self.n_params} parameters\")\n",
    "        \n",
    "    def predict_next_token(self, decoder_in):\n",
    "        decoder_out = self.decode(decoder_in)\n",
    "        last_token = decoder_out[-1]\n",
    "        \n",
    "        pred_out = (last_token @ self.projection).softmax(dim=0)\n",
    "        next_idx = torch.argmax(pred_out)\n",
    "        return self.i_to_v[next_idx.item()]    \n",
    "    \n",
    "    def predict_all(self, decoder_in):\n",
    "        decoder_out = self.decode(decoder_in)\n",
    "        pred_out = (decoder_out @ self.projection).softmax(dim=1)\n",
    "        return pred_out\n",
    "        \n",
    "    def encode(self, inp):\n",
    "        encoder_outs = torch.empty((self.n_stack, inp.shape[0], inp.shape[1]))\n",
    "        for (idx, encoder_param_group) in enumerate(self.encoder_params):\n",
    "            [mha_params, ln_1_params, ff_params, ln_2_params] = self.__interpret_encoder_params(encoder_param_group)\n",
    "            \n",
    "            # multi-head attention\n",
    "            mha = self.__multi_head_attention(inp, inp, inp, mha_params, False)\n",
    "            \n",
    "            # add & norm\n",
    "            mha_out = self.__layer_norm(mha + inp, ln_1_params)\n",
    "            \n",
    "            # feed-forward\n",
    "            ff = self.__feed_forward(mha_out, ff_params)\n",
    "            \n",
    "            # add & norm\n",
    "            encoder_outs[idx] = self.__layer_norm(ff + mha_out, ln_2_params)\n",
    "            \n",
    "        self.encoder_outs = encoder_outs\n",
    "        return self.encoder_outs\n",
    "    \n",
    "    def decode(self, inp):\n",
    "        if(self.n_stack > self.encoder_outs.shape[0]):\n",
    "            raise Exception(\"Encode before decoding\")\n",
    "            \n",
    "        stack_in = inp\n",
    "        for (idx, decoder_param_group) in enumerate(self.decoder_params):\n",
    "            [ \n",
    "                masked_mha_params,\n",
    "                ln_1_params,\n",
    "                mha_params,\n",
    "                ln_2_params,\n",
    "                ff_params,\n",
    "                ln_3_params\n",
    "            ] = self.__interpret_decoder_params(decoder_param_group)\n",
    "            \n",
    "            # masked multi-head attention\n",
    "            masked_mha = self.__multi_head_attention(stack_in, stack_in, stack_in, masked_mha_params, True)\n",
    "            \n",
    "            # add & norm\n",
    "            masked_mha_out= self.__layer_norm(masked_mha + stack_in, ln_1_params)\n",
    "            \n",
    "            # multi-head attention (Q - from masked_mha_out, K - from encoder, V - from encoder)\n",
    "            mha = self.__multi_head_attention(masked_mha_out, self.encoder_outs[idx], self.encoder_outs[idx], mha_params, False)\n",
    "            \n",
    "            # add & norm\n",
    "            mha_out = self.__layer_norm(mha + masked_mha_out, ln_2_params)\n",
    "            \n",
    "            # feed-forward\n",
    "            ff = self.__feed_forward(mha_out, ff_params)\n",
    "            \n",
    "            # add & norm\n",
    "            stack_in = self.__layer_norm(ff + mha_out, ln_3_params)\n",
    "        return stack_in\n",
    "            \n",
    "    def __feed_forward(self, inp, ff_params):\n",
    "        [W1, b1, W2, b2] = ff_params\n",
    "        return (((inp @ W1) + b1).relu() @ W2) + b2\n",
    "    \n",
    "    def __layer_norm(self, inp, ln_params):\n",
    "        [scale, bias] = ln_params\n",
    "        mean = inp.mean(dim=1).unsqueeze(dim=1)\n",
    "        std = inp.std(dim=1).unsqueeze(dim=1)\n",
    "\n",
    "        norm = (inp - mean)/std\n",
    "\n",
    "        return (norm * scale) + bias\n",
    "    \n",
    "    def __multi_head_attention(self, q_inp, k_inp, v_inp, params, masked=False):\n",
    "        W_o = params[0]\n",
    "        W_qkv_groups = params[1:]\n",
    "        \n",
    "        if(len(W_qkv_groups) != self.n_heads):\n",
    "            raise Exception(f\"Params doesn't match num of heads. Expected: {self.n_heads}. Found: {len(W_qkv_groups)}\")\n",
    "            \n",
    "        multi_head_out = torch.tensor([])\n",
    "        for head_idx in range(self.n_heads):\n",
    "            [W_q, W_k, W_v] = W_qkv_groups[head_idx]        \n",
    "            head_out = self.__single_head_attention(q_inp, k_inp, v_inp, W_q, W_k, W_v, masked)\n",
    "            multi_head_out = torch.cat([multi_head_out, head_out], dim=1)\n",
    "            \n",
    "        return multi_head_out @ W_o\n",
    "    \n",
    "    def __single_head_attention(self, q_inp, k_inp, v_inp, W_q, W_k, W_v, masked):\n",
    "        # linear layers\n",
    "        Q = q_inp @ W_q\n",
    "        K = k_inp @ W_k\n",
    "        V = v_inp @ W_v\n",
    "        \n",
    "        # scaled dot-product attention\n",
    "        mat_mul = Q @ K.T\n",
    "        if(masked):\n",
    "            mask = torch.tril(torch.ones(mat_mul.shape))\n",
    "            mat_mul = torch.where(mask == 0, float(\"-inf\"), mat_mul)\n",
    "            \n",
    "        scale = mat_mul/torch.sqrt(self.d_head_t)\n",
    "        softmax = torch.nn.functional.softmax(scale, dim=None)\n",
    "        return softmax @ V\n",
    "        \n",
    "    def embed_seq(self, seq):\n",
    "        one_hot = torch.nn.functional.one_hot(torch.tensor([self.v_to_i[v] for v in seq]), self.vocab_size).float()\n",
    "        embedded = one_hot @ self.embeddings\n",
    "        return torch.stack([emb + self.__get_positional_encoding(pos) for (pos, emb) in enumerate(embedded)])\n",
    "        \n",
    "    def __get_positional_encoding(self, pos):\n",
    "        v_dims = torch.arange(0, (self.d_model+1)/2, step=1, dtype=torch.float)\n",
    "\n",
    "        def get_pe(v):\n",
    "            exponent = (2*v)/self.d_model\n",
    "            return pos/torch.pow(10000,exponent)\n",
    "\n",
    "        pre_sinusoid = get_pe(v_dims)\n",
    "        pe_even = pre_sinusoid.sin()\n",
    "        pe_odd = pre_sinusoid.cos() \n",
    "\n",
    "        pe = torch.stack([pe_even, pe_odd])\n",
    "\n",
    "        return pe.mT.reshape(-1)[:self.d_model]\n",
    "    \n",
    "    def __generate_encoder_params(self):\n",
    "        return [\n",
    "            ### attention ###\n",
    "            # linear\n",
    "            torch.randn((self.d_model, self.d_model), dtype=torch.float),\n",
    "            # heads\n",
    "            *[torch.randn((self.d_model, self.d_head), dtype=torch.float) for _ in range(3) for _ in range(self.n_heads)],\n",
    "            ### layer norm 1 ###\n",
    "            torch.randn((self.d_model), dtype=torch.float),\n",
    "            torch.randn((self.d_model), dtype=torch.float),\n",
    "            ### feed forward ###\n",
    "            # W1\n",
    "            torch.randn((self.d_model, self.ff_hidden_size), dtype=torch.float),\n",
    "            # B1\n",
    "            torch.randn((self.ff_hidden_size), dtype=torch.float),\n",
    "            # W2\n",
    "            torch.randn((self.ff_hidden_size, self.d_model), dtype=torch.float),\n",
    "            # B4\n",
    "            torch.randn((self.d_model), dtype=torch.float),\n",
    "            ### layer norm 2 ###\n",
    "            torch.randn((self.d_model), dtype=torch.float),\n",
    "            torch.randn((self.d_model), dtype=torch.float),\n",
    "        ]\n",
    "    \n",
    "    def __interpret_encoder_params(self, param_group):\n",
    "        attention_params = param_group[:(3*self.n_heads)+1]\n",
    "        param_group = param_group[(3*self.n_heads)+1:]\n",
    "        \n",
    "        grouped_heads = [attention_params[i+1: i+4] for i in range(0, 3*self.n_heads, 3)]\n",
    "        \n",
    "        layer_norm_1_params = param_group[:2]\n",
    "        param_group = param_group[2:]\n",
    "        \n",
    "        ff_params = param_group[:4]\n",
    "        param_group = param_group[4:]\n",
    "        \n",
    "        layer_norm_2_params = param_group[:2]\n",
    "        param_group = param_group[2:]\n",
    "        \n",
    "        return [\n",
    "            # attention group\n",
    "            [\n",
    "                attention_params[0],\n",
    "                *grouped_heads\n",
    "            ],\n",
    "            # layer norm 1 group\n",
    "            layer_norm_1_params,\n",
    "            # feed forward group\n",
    "            ff_params,\n",
    "            # layer norm 2 group\n",
    "            layer_norm_2_params\n",
    "        ]\n",
    "    \n",
    "    def __generate_decoder_params(self):\n",
    "        return [\n",
    "            ### masked attention ###\n",
    "            # linear\n",
    "            torch.randn((self.d_model, self.d_model), dtype=torch.float),\n",
    "            # heads\n",
    "            *[torch.randn((self.d_model, self.d_head), dtype=torch.float) for _ in range(3) for _ in range(self.n_heads)],\n",
    "            ### layer norm 1 ###\n",
    "            torch.randn((self.d_model), dtype=torch.float),\n",
    "            torch.randn((self.d_model), dtype=torch.float),\n",
    "            ### attention ###\n",
    "            # linear\n",
    "            torch.randn((self.d_model, self.d_model), dtype=torch.float),\n",
    "            # heads\n",
    "            *[torch.randn((self.d_model, self.d_head), dtype=torch.float) for _ in range(3) for _ in range(self.n_heads)],\n",
    "            ### layer norm 2 ###\n",
    "            torch.randn((self.d_model), dtype=torch.float),\n",
    "            torch.randn((self.d_model), dtype=torch.float),\n",
    "            ### feed forward ###\n",
    "            # W1\n",
    "            torch.randn((self.d_model, self.ff_hidden_size), dtype=torch.float),\n",
    "            # B1\n",
    "            torch.randn((self.ff_hidden_size), dtype=torch.float),\n",
    "            # W2\n",
    "            torch.randn((self.ff_hidden_size, self.d_model), dtype=torch.float),\n",
    "            # B4\n",
    "            torch.randn((self.d_model), dtype=torch.float),\n",
    "            ### layer norm 3 ###\n",
    "            torch.randn((self.d_model), dtype=torch.float),\n",
    "            torch.randn((self.d_model), dtype=torch.float),\n",
    "        ]\n",
    "    \n",
    "    def __interpret_decoder_params(self, param_group):\n",
    "        masked_attention_params = param_group[:(3*self.n_heads)+1]\n",
    "        param_group = param_group[(3*self.n_heads)+1:]\n",
    "        \n",
    "        grouped_masked_heads = [masked_attention_params[i+1: i+4] for i in range(0, 3*self.n_heads, 3)]\n",
    "        \n",
    "        layer_norm_1_params = param_group[:2]\n",
    "        param_group = param_group[2:]\n",
    "        \n",
    "        attention_params = param_group[:(3*self.n_heads)+1]\n",
    "        param_group = param_group[(3*self.n_heads)+1:]\n",
    "        \n",
    "        grouped_heads = [attention_params[i+1: i+4] for i in range(0, 3*self.n_heads, 3)]\n",
    "        \n",
    "        layer_norm_2_params = param_group[:2]\n",
    "        param_group = param_group[2:]\n",
    "                \n",
    "        ff_params = param_group[:4]\n",
    "        param_group = param_group[4:]\n",
    "        \n",
    "        layer_norm_3_params = param_group[:2]\n",
    "        param_group = param_group[2:]\n",
    "        \n",
    "        \n",
    "        return [\n",
    "            # masked attention group\n",
    "            [\n",
    "                masked_attention_params[0],\n",
    "                *grouped_masked_heads\n",
    "            ],\n",
    "            # layer norm 1 group\n",
    "            layer_norm_1_params,\n",
    "            # attention group\n",
    "            [\n",
    "                attention_params[0],\n",
    "                *grouped_heads\n",
    "            ],\n",
    "            # layer norm 2 group\n",
    "            layer_norm_2_params,\n",
    "            # feed forward group\n",
    "            ff_params,\n",
    "            # layer norm 3 group\n",
    "            layer_norm_3_params\n",
    "        ]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "73993b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_names = open(\"names.txt\").read().split(\"\\n\")\n",
    "names = [list(n) for n in raw_names]\n",
    "\n",
    "open_token = \"<o>\"\n",
    "close_token = \"<c>\"\n",
    "vocab = sorted(set([open_token, close_token] + [ch for name in names for ch in name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "id": "32bcd39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 100\n",
    "X = names[:dataset_size]\n",
    "Y = [[open_token] + n[::-1] + [close_token] for n in names[:dataset_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "id": "33cf1430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer created with 264 parameters\n"
     ]
    }
   ],
   "source": [
    "transformer = Transformer(2, vocab, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "id": "72516ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encode_emb = [transformer.embed_seq(x) for x in X]\n",
    "X_decode_emb = [transformer.embed_seq(y) for y in Y]\n",
    "\n",
    "def one_hot_target(seq, transformer):\n",
    "    one_hot = torch.nn.functional.one_hot(torch.tensor([transformer.v_to_i[v] for v in seq]), transformer.vocab_size).float()\n",
    "    return one_hot\n",
    "\n",
    "Y_targets = [one_hot_target(y, transformer)[1:] for y in Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "b7bd3590",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adamax(transformer.params, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "fae1b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.2508:   5%|█▎                         | 48/1000 [00:01<00:38, 24.57it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qb/1t4g8ttj0y7493wrr_2myvqw0000gn/T/ipykernel_2829/2455657669.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mx_decode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_encode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_decode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/qb/1t4g8ttj0y7493wrr_2myvqw0000gn/T/ipykernel_2829/665387531.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m# multi-head attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mmha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__multi_head_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmha_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# add & norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/qb/1t4g8ttj0y7493wrr_2myvqw0000gn/T/ipykernel_2829/665387531.py\u001b[0m in \u001b[0;36m__multi_head_attention\u001b[0;34m(self, q_inp, k_inp, v_inp, params, masked)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Params doesn't match num of heads. Expected: {self.n_heads}. Found: {len(W_qkv_groups)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mmulti_head_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhead_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mW_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_v\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW_qkv_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhead_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with trange(1000) as pbar:\n",
    "    for it in pbar:\n",
    "        batch_size = 16\n",
    "        ix = torch.randint(0, dataset_size, (batch_size,))\n",
    "\n",
    "        optimiser.zero_grad(set_to_none=True)\n",
    "        \n",
    "        loss = 0\n",
    "        for idx in ix:\n",
    "            x_encode = transformer.embed_seq(X[idx])\n",
    "            x_decode = transformer.embed_seq(Y[idx])\n",
    "\n",
    "            transformer.encode(x_encode)\n",
    "\n",
    "            out = transformer.predict_all(x_decode)\n",
    "            loss += cross_entropy_loss(out[:-1], Y_targets[idx])\n",
    "\n",
    "        loss /= batch_size\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimiser.step()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "a2686e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.1535e-32)\n",
      "tensor(-8.9768e-31)\n",
      "tensor(1.3967e-31)\n",
      "tensor(6.0826e-32)\n",
      "tensor(-4.4275e-25)\n",
      "tensor(5.7013e-25)\n",
      "tensor(4.8852e-26)\n",
      "tensor(2.9182e-26)\n",
      "tensor(5.5932e-25)\n",
      "tensor(5.0250e-25)\n",
      "tensor(-7.6124e-19)\n",
      "tensor(5.2250e-18)\n",
      "tensor(-6.8561e-22)\n",
      "tensor(-6.0481e-22)\n",
      "tensor(1.0923e-22)\n",
      "tensor(4.3129e-22)\n",
      "tensor(-2.5657e-18)\n",
      "tensor(-1.6265e-18)\n",
      "tensor(1.6910e-18)\n",
      "tensor(-4.3059e-19)\n",
      "tensor(-4.0898e-19)\n",
      "tensor(-6.3976e-19)\n",
      "tensor(3.3482e-11)\n",
      "tensor(4.5965e-11)\n",
      "tensor(-3.3333e-12)\n",
      "tensor(-1.4998e-12)\n",
      "tensor(1.4114e-11)\n",
      "tensor(1.2960e-11)\n",
      "tensor(-6.6900e-06)\n",
      "tensor(-0.0002)\n",
      "tensor(6.0268e-24)\n",
      "tensor(7.0575e-11)\n"
     ]
    }
   ],
   "source": [
    "for p in transformer.params:\n",
    "    print(p.grad.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "id": "f72c16cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a']\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n"
     ]
    }
   ],
   "source": [
    "transformer.encode(transformer.embed_seq(names[0]))\n",
    "print(names[0])\n",
    "ch = open_token\n",
    "word = [ch]\n",
    "while(ch != close_token and len(word) < 100):\n",
    "    ch = transformer.predict_next_token(transformer.embed_seq([open_token]))\n",
    "    word += [ch]\n",
    "\n",
    "print(word[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b07dcf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
