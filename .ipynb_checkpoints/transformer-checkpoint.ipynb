{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "00648b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "8c621b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, masked=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.d_head_t = torch.tensor(self.d_head)\n",
    "        self.masked = masked\n",
    "        \n",
    "        assert(self.d_head * self.n_heads == self.d_model)\n",
    "        \n",
    "        self.qkv = nn.ModuleList([nn.ModuleList([nn.Linear(self.d_model, self.d_head, bias=False)]*3)]*self.n_heads)\n",
    "        \n",
    "        self.fc_out = nn.Linear(self.d_model, self.d_model, bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, q_in, k_in, v_in):\n",
    "        self.mask = torch.ones((q_in.shape[0], k_in.shape[0]))\n",
    "        if(self.masked):\n",
    "            self.mask = torch.tril(self.mask)\n",
    "            \n",
    "        self_attention_out = torch.tensor([])\n",
    "        for [q_linear, k_linear, v_linear] in self.qkv:\n",
    "            Q = q_linear(q_in)\n",
    "            K = k_linear(k_in)\n",
    "            V = v_linear(v_in)\n",
    "            \n",
    "            mat_mul = (Q @ K.T).masked_fill(self.mask == 0, float(\"-inf\"))\n",
    "            scale = mat_mul/torch.sqrt(self.d_head_t)\n",
    "            softmax = torch.nn.functional.softmax(scale, dim=None)\n",
    "            scaled_dot_prod_attention = softmax @ V\n",
    "            \n",
    "            self_attention_out = torch.cat([self_attention_out, scaled_dot_prod_attention], dim=1)\n",
    "                \n",
    "        \n",
    "        out = self.fc_out(self_attention_out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "82b90e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout, ff_hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.attention = SelfAttention(self.d_model, self.n_heads, False)\n",
    "        self.norm1 = nn.LayerNorm(self.d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(self.d_model, ff_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_size, self.d_model)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(self.d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, block_in):\n",
    "        # self attention\n",
    "        sa = self.attention(block_in, block_in, block_in)\n",
    "        \n",
    "        # add & norm\n",
    "        sa_res_norm = self.dropout(self.norm1(sa + block_in))\n",
    "        \n",
    "        # feed-forward\n",
    "        ff = self.ff(sa_res_norm)\n",
    "        \n",
    "        # add & norm\n",
    "        ff_res_norm = self.dropout(self.norm2(ff + sa_res_norm))\n",
    "        \n",
    "        return ff_res_norm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "6f42cd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, n_stack, vocab_size, n_heads, dropout, ff_hidden_size, max_length):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.d_model = d_model\n",
    "        self.n_stack = n_stack\n",
    "        self.n_heads = n_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.seq_embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        self.positional_embedding = nn.Embedding(self.max_length, self.d_model)\n",
    "        \n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                EncoderBlock(self.d_model, self.n_heads, dropout, ff_hidden_size)\n",
    "            ] * self.n_stack\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        out = self.seq_embedding(x) + self.positional_embedding(torch.arange(0, x.shape[0]))\n",
    "        \n",
    "        for block in self.encoder_blocks:\n",
    "            out = block(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "f1eb7c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout, ff_hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.masked_attention = SelfAttention(self.d_model, self.n_heads, True)\n",
    "        self.norm1 = nn.LayerNorm(self.d_model)\n",
    "        \n",
    "        self.attention = SelfAttention(self.d_model, self.n_heads, True)\n",
    "        self.norm2 = nn.LayerNorm(self.d_model)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(self.d_model, ff_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_size, self.d_model)\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(self.d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, enc_out, block_in):\n",
    "        # masked self attention\n",
    "        msa = self.masked_attention(block_in, block_in, block_in)\n",
    "        \n",
    "        # add & norm\n",
    "        msa_res_norm = self.dropout(self.norm1(msa + block_in))\n",
    "        \n",
    "        # self attention\n",
    "        sa = self.attention(block_in, enc_out, enc_out)\n",
    "        \n",
    "        # add & norm\n",
    "        sa_res_norm = self.dropout(self.norm2(sa + msa_res_norm))\n",
    "        \n",
    "        # feed-forward\n",
    "        ff = self.ff(sa_res_norm)\n",
    "        \n",
    "        # add & norm\n",
    "        ff_res_norm = self.dropout(self.norm3(ff + sa_res_norm))\n",
    "        \n",
    "        return ff_res_norm  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "6e8fdaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, n_stack, vocab_size, n_heads, dropout, ff_hidden_size, max_length):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_stack = n_stack\n",
    "        self.n_heads = n_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.seq_embedding = nn.Embedding(self.vocab_size, self.d_model)\n",
    "        self.positional_embedding = nn.Embedding(self.max_length, self.d_model)\n",
    "        \n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(self.d_model, self.n_heads, dropout, ff_hidden_size)\n",
    "            ] * self.n_stack\n",
    "        )\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, src, x):\n",
    "        out = self.seq_embedding(x) + self.positional_embedding(torch.arange(0, x.shape[0]))\n",
    "        \n",
    "        for block in self.decoder_blocks:\n",
    "            out = block(src, out)\n",
    "            \n",
    "        out = self.fc_out(out)\n",
    "        \n",
    "        return out.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "0d61cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, n_stack, vocab_size, n_heads, dropout, ff_hidden_size, max_length=512):\n",
    "        super().__init__()      \n",
    "        \n",
    "        self.encoder = Encoder(d_model, n_stack, vocab_size, n_heads, dropout, ff_hidden_size, max_length)\n",
    "        \n",
    "        self.decoder = Decoder(d_model, n_stack, vocab_size, n_heads, dropout, ff_hidden_size, max_length)\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        enc_out = self.encoder(src) \n",
    "        \n",
    "        out = self.decoder(enc_out, trg)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "95cad4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_names = open(\"names.txt\").read().split(\"\\n\")\n",
    "names = [list(n) for n in raw_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "33374a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_token = \"<o>\"\n",
    "close_token = \"<c>\"\n",
    "vocab = sorted(set([open_token, close_token] + [ch for name in names for ch in name]))\n",
    "\n",
    "ch_to_i = {ch:idx for (idx, ch) in enumerate(vocab)}\n",
    "i_to_ch = {idx:ch for (idx, ch) in enumerate(vocab)}\n",
    "\n",
    "encoded_names = [[ch_to_i[ch] for ch in name] for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "7fa352ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 16\n",
    "n_stack = 2\n",
    "vocab_size = len(vocab)\n",
    "n_heads = 4\n",
    "dropout = 0.95\n",
    "t = Transformer(d_model, n_stack, vocab_size, n_heads, dropout, d_model*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "7bc8ca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23132 parameters\n"
     ]
    }
   ],
   "source": [
    "parameters = t.parameters()\n",
    "print(f\"{sum([p.nelement() for p in parameters])} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "5f6d3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_inputs = [torch.tensor(name) for name in encoded_names]\n",
    "dec_inputs = [torch.tensor([ch_to_i[open_token]] + enc_name) for enc_name in encoded_names]\n",
    "exp_out = [torch.tensor(enc_name + [ch_to_i[close_token]]) for enc_name in encoded_names]\n",
    "exp_out = [torch.nn.functional.one_hot(i, vocab_size).float() for i in exp_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "bb862c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_loss = torch.nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adamax(t.parameters())\n",
    "\n",
    "dataset_size = len(encoded_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "30a75513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 3.3288:  49%|████████████▋             | 490/1000 [00:45<00:47, 10.78it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qb/1t4g8ttj0y7493wrr_2myvqw0000gn/T/ipykernel_5452/2811650280.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loss: {loss.item():.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with trange(1000) as pbar:\n",
    "    for it in pbar:\n",
    "        batch_size = 64\n",
    "        ix = torch.randint(0, dataset_size, (batch_size,))\n",
    "        \n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        loss = 0\n",
    "        for idx in ix:\n",
    "            out = t(enc_inputs[idx], dec_inputs[idx])\n",
    "            loss += cross_entropy_loss(out, exp_out[idx])\n",
    "\n",
    "        loss /= batch_size\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimiser.step()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06381e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
