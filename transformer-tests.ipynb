{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "12855495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6a2e6c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_names = open(\"names.txt\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0daaae09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['e', 'm', 'm', 'a'],\n",
       " ['o', 'l', 'i', 'v', 'i', 'a'],\n",
       " ['a', 'v', 'a'],\n",
       " ['i', 's', 'a', 'b', 'e', 'l', 'l', 'a'],\n",
       " ['s', 'o', 'p', 'h', 'i', 'a']]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [list(n) for n in raw_names]\n",
    "names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9dec2b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "ch_to_i = {ch:idx for (idx, ch) in enumerate(vocab)}\n",
    "i_to_ch = {idx:ch for (idx, ch) in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "df8a43d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# char embeddings\n",
    "d_model = torch.tensor(16)\n",
    "embeddings = torch.randn((vocab_size, d_model), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "875abd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding\n",
    "\n",
    "def get_positional_encoding(pos, model_dims):\n",
    "    v_dims = torch.arange(0, (model_dims+1)/2, step=1, dtype=torch.float)\n",
    "    \n",
    "    def get_pe(v):\n",
    "        exponent = (2*v)/model_dims\n",
    "        return pos/torch.pow(10000,exponent)\n",
    "    \n",
    "    pre_sinusoid = get_pe(v_dims)\n",
    "    pe_even = pre_sinusoid.sin()\n",
    "    pe_odd = pre_sinusoid.cos() \n",
    "    \n",
    "    pe = torch.stack([pe_even, pe_odd])\n",
    "    \n",
    "    return pe.mT.reshape(-1)[:model_dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2d44da59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_name(name):\n",
    "    one_hot = torch.nn.functional.one_hot(torch.tensor([ch_to_i[ch] for ch in name]), vocab_size).float()\n",
    "    \n",
    "    embedded_chars = one_hot @ embeddings\n",
    "    \n",
    "    encoded_name = torch.stack([emb + get_positional_encoding(pos, d_model) for (pos, emb) in enumerate(embedded_chars)])\n",
    "    \n",
    "    return encoded_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ddd726eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single head attention\n",
    "def single_head_attention(q_in, k_in, v_in, q_l, k_l, v_l, scale_dims, masked):\n",
    "    # linear layers\n",
    "    Q = q_in @ q_l\n",
    "    K = k_in @ k_l\n",
    "    V = v_in @ v_l\n",
    "    \n",
    "    # scaled dot-product attention\n",
    "    mat_mul = Q @ K.T\n",
    "    if(masked):\n",
    "        mask = torch.tril(torch.ones(mat_mul.shape))\n",
    "        mat_mul = torch.where(mask == 0, float(\"-inf\"), mat_mul)\n",
    "    scale = mat_mul/torch.sqrt(scale_dims)\n",
    "    softmax = torch.nn.functional.softmax(scale, dim=None)\n",
    "    return softmax @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "adaf6c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(q_in, k_in, v_in, n_heads, params, masked=False):\n",
    "    out_params = params[0]\n",
    "    qkv_param_groups = params[1:]\n",
    "\n",
    "    if(len(qkv_param_groups) != n_heads):\n",
    "        raise Exception(f\"Params doesn't match num of heads. Expected: {n_heads}. Found: {len(qkv_param_groups)}\")\n",
    "        \n",
    "    head_dims = d_model / n_heads;\n",
    "    \n",
    "    head_results = torch.tensor([])\n",
    "    for head_idx in range(n_heads):\n",
    "        [q_l, k_l, v_l] = qkv_param_groups[head_idx]        \n",
    "        head_res = single_head_attention(q_in, k_in, v_in, q_l, k_l, v_l, head_dims, masked)\n",
    "        \n",
    "        head_results = torch.cat([head_results, head_res], dim=1)\n",
    "        \n",
    "    # final linear layer\n",
    "    return head_results @ out_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "94f81372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qkv_mats(model_dims, head_dims):\n",
    "    model_dims = model_dims.int()\n",
    "    head_dims = head_dims.int()\n",
    "    return [\n",
    "        torch.randn((model_dims, head_dims)),\n",
    "        torch.randn((model_dims, head_dims)),\n",
    "        torch.randn((model_dims, head_dims)),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "c3083519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(inp, scale, bias):\n",
    "    mean = inp.mean(dim=1).unsqueeze(dim=1)\n",
    "    std = inp.std(dim=1).unsqueeze(dim=1)\n",
    "    \n",
    "    norm = (inp - mean)/std\n",
    "    \n",
    "    return (norm * scale) + bias\n",
    "\n",
    "def transformer_encoder_single(d_model, n_heads, encoded_input, params):\n",
    "    # separate params\n",
    "    ff_params = params[:4]\n",
    "    ln_params = params[4: 8]\n",
    "    head_params = params[8:]\n",
    "    \n",
    "    [ln_s1, ln_b1, ln_s2, ln_b2] = ln_params\n",
    "\n",
    "    # multi-head attention\n",
    "    mha = multi_head_attention(encoded_input, encoded_input, encoded_input, n_heads, head_params, False)\n",
    "    \n",
    "    # residual connection\n",
    "    mha_res = layer_norm(mha + encoded_input, ln_s1, ln_b1)\n",
    "    \n",
    "    # feed-forward\n",
    "    [W1, b1, W2, b2] = ff_params\n",
    "    ff = (((mha_res @ W1) + b1).relu() @ W2) + b2\n",
    "    \n",
    "    # residual connection and norm\n",
    "    return layer_norm(ff + mha_res, ln_s2, ln_b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "58d51164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output becomes values (V) and keys (K) to be queried (Q) by decoder input.\n",
    "\n",
    "def transformer_encoder_stack(d_model, n_encoders, n_heads, encoded_input, param_groups):\n",
    "    if(len(param_groups) != n_encoders):\n",
    "        raise Exception(\"n_encoders not compatible with params\")\n",
    "        \n",
    "    ff_h_size = d_model*4\n",
    "    \n",
    "    # feed through stack in sequence\n",
    "    encoder_outputs = []\n",
    "    res = encoded_input\n",
    "    for param_group in param_groups:\n",
    "        res = transformer_encoder_single(d_model, n_heads, res, param_group)\n",
    "        encoder_outputs += [res]\n",
    "    return encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "cc42b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_decoder_single(d_model, n_masked_heads, n_heads, encoded_input, encoder_output, params):\n",
    "    # separate params\n",
    "    \n",
    "    # take masked head params\n",
    "    masked_head_params = params[:n_masked_heads+1]\n",
    "    params = params[n_masked_heads+1:]\n",
    "    \n",
    "    # take first layer norm params\n",
    "    [ln_1_scale, ln_1_bias] = params[:2]\n",
    "    params = params[2:]\n",
    "    \n",
    "    # take head params\n",
    "    head_params = params[:n_heads+1]\n",
    "    params = params[n_heads+1:]\n",
    "    \n",
    "    # take second layer norm params\n",
    "    [ln_2_scale, ln_2_bias] = params[:2]\n",
    "    params = params[2:]\n",
    "    \n",
    "    # feed-forward params\n",
    "    [W1, b1, W2, b2] = params[:4]\n",
    "    params = params[4:]\n",
    "    \n",
    "    # take third layer norm params\n",
    "    [ln_3_scale, ln_3_bias] = params[:2]\n",
    "    params = params[2:]\n",
    "    \n",
    "    if(len(params) != 0):\n",
    "        raise Exception(f\"Params mismatch, {len(params)} left\")\n",
    "    \n",
    "    # masked multi-head\n",
    "    masked_mha = multi_head_attention(encoded_input, encoded_input, encoded_input, n_masked_heads, masked_head_params, True)\n",
    "    # residual connection\n",
    "    masked_mha_res = masked_mha + encoded_input\n",
    "    # layer norm\n",
    "    masked_mha_out = layer_norm(masked_mha_res, ln_1_scale, ln_1_bias)\n",
    "    \n",
    "    # multi-head (Q - masked_mha_out, K and V - corresponding encoder output)\n",
    "    mha = multi_head_attention(masked_mha_out, encoder_output, encoder_output, n_heads, head_params, False)\n",
    "    # residual connection\n",
    "    mha_res = mha + masked_mha_out\n",
    "    # layer norm\n",
    "    mha_out = layer_norm(mha_res, ln_2_scale, ln_2_bias)\n",
    "    \n",
    "    # feed-forward\n",
    "    ff = (((mha_out @ W1) + b1).relu() @ W2) + b2\n",
    "    # residual connection\n",
    "    ff_res = ff + mha_out\n",
    "    # layer norm\n",
    "    return layer_norm(ff_res, ln_3_scale, ln_3_bias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "de8c33d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qb/1t4g8ttj0y7493wrr_2myvqw0000gn/T/ipykernel_2323/1360203101.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmax = torch.nn.functional.softmax(scale, dim=None)\n"
     ]
    }
   ],
   "source": [
    "encoding = encode_name(names[3])\n",
    "\n",
    "ff_h_size = d_model*4\n",
    "\n",
    "encoder_params = [\n",
    "    [\n",
    "        # feed-foward params\n",
    "        # W1\n",
    "        torch.randn((d_model, ff_h_size)),\n",
    "        # B1\n",
    "        torch.randn((ff_h_size)),\n",
    "        # W2\n",
    "        torch.randn((ff_h_size, d_model)),\n",
    "        # B4\n",
    "        torch.randn((d_model)),\n",
    "        # layer norm params\n",
    "        torch.randn((d_model)),\n",
    "        torch.randn((d_model)),\n",
    "        torch.randn((d_model)),\n",
    "        torch.randn((d_model)),\n",
    "        # multi-head attention params\n",
    "        torch.randn((d_model, d_model)),\n",
    "        generate_qkv_mats(d_model, d_model/4),\n",
    "        generate_qkv_mats(d_model, d_model/4),\n",
    "        generate_qkv_mats(d_model, d_model/4),\n",
    "        generate_qkv_mats(d_model, d_model/4)\n",
    "    ],\n",
    "]\n",
    "\n",
    "decoder_params = [\n",
    "    # masked_head_params\n",
    "    torch.randn((d_model, d_model)),\n",
    "    generate_qkv_mats(d_model, d_model/4),\n",
    "    generate_qkv_mats(d_model, d_model/4),\n",
    "    generate_qkv_mats(d_model, d_model/4),\n",
    "    generate_qkv_mats(d_model, d_model/4),\n",
    "    # layer norm params\n",
    "    torch.randn((d_model)),\n",
    "    torch.randn((d_model)),\n",
    "    # head params\n",
    "    torch.randn((d_model, d_model)),\n",
    "    generate_qkv_mats(d_model, d_model/4),\n",
    "    generate_qkv_mats(d_model, d_model/4),\n",
    "    generate_qkv_mats(d_model, d_model/4),\n",
    "    generate_qkv_mats(d_model, d_model/4),\n",
    "    # layer norm params\n",
    "    torch.randn((d_model)),\n",
    "    torch.randn((d_model)),\n",
    "    # feed-foward params\n",
    "    # W1\n",
    "    torch.randn((d_model, ff_h_size)),\n",
    "    # B1\n",
    "    torch.randn((ff_h_size)),\n",
    "    # W2\n",
    "    torch.randn((ff_h_size, d_model)),\n",
    "    # B4\n",
    "    torch.randn((d_model)),\n",
    "    # layer norm params\n",
    "    torch.randn((d_model)),\n",
    "    torch.randn((d_model)),\n",
    "]\n",
    "\n",
    "encoder_result = transformer_encoder_stack(d_model, 1, 4, encoding, encoder_params)\n",
    "decoder_result = transformer_decoder_single(d_model, 4, 4, encoding, encoder_result[0], decoder_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
